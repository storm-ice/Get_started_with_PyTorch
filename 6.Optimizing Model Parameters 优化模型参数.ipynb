{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d65a7d1-40f6-4345-aab3-49dca4e0e0ec",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters 优化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d2f22-8a7b-44c6-8e73-651ab4451ea3",
   "metadata": {},
   "source": [
    "现在我们有了模型和数据，是时候通过优化数据上的参数来训练、验证和测试我们的模型了。训练模型是一个迭代过程；在每次迭代中，模型都会对输出进行猜测，计算其猜测中的误差（*损失*），收集相对于其参数的导数的误差（如我们在[上一节](https://pytorch.org/tutorials/beginner/basics/autograd_tutorial.html)中看到的），并使用梯度下降**优化**这些参数。有关此过程的更详细演练，请观看[3Blue1Brown 的反向传播](https://www.youtube.com/watch?v=tIeHLnjs5U8)有关视频。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c123f9f-c771-4c69-9ab0-84669084a147",
   "metadata": {},
   "source": [
    "# Prerequisite Code 前置代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ca765-defa-42e1-b1cf-9d7ecabe0c69",
   "metadata": {},
   "source": [
    "我们加载前面有关[数据集和数据加载器](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) 以及[构建模型](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a8152c-9cb6-4af1-b959-c507f2f62bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = 64)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45222a02-5a4c-485f-b62a-f27b2d2893bf",
   "metadata": {},
   "source": [
    "# Hyperpatameters 超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82fdf14-7cf0-4b01-b499-f99a95d41a47",
   "metadata": {},
   "source": [
    "超参数是可调整的参数，可让您控制模型优化过程。不同的超参数值会影响模型训练和收敛速度（[阅读](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)有关超参数调整的更多信息）\r\n",
    "\r\n",
    "我们定义以下训练超参数：\r\n",
    "\r\n",
    "-  **Number of Epochs** - 迭代数据集的次数\r\n",
    "-  **Batch Size** - 参数更新之前，通过网络传播的数据样本数量\r\n",
    "-  **Learning Rate**- 每个batch/epoch更新模型参数的量。较小的值会导致学习速度较慢，而较大的值可能会导致训练期间出现不可预测的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46340230-38e3-4b18-8827-dd7b7e8539bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f302e-da31-4775-940a-029848f60b26",
   "metadata": {},
   "source": [
    "# Optimization Loop 优化循环"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529efeb1-8996-4a22-8dd6-b9737047bb1b",
   "metadata": {},
   "source": [
    "一旦我们设置了超参数，我们就可以使用优化循环来训练和优化我们的模型。优化循环的每次迭代称为一个**epoch**。\r\n",
    "\r\n",
    "每个 epoch由两个主要部分组成：\r\n",
    "\r\n",
    "-  **The Train Loop**- 迭代训练数据集并尝试收敛到最佳参数。\r\n",
    "-  **The Validation/Test Loop **- 迭代测试数据集以检查模型性能是否有所改善。\r\n",
    "\r\n",
    "让我们简单熟悉一下训练循环中使用的一些概念。向前跳转查看优化循环的[完整实现](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-impl-label)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd39d24-73af-449b-a55c-d3836c949838",
   "metadata": {},
   "source": [
    "## Loss Funtion 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d6f9b4-7508-4266-80a5-320606f18d0c",
   "metadata": {},
   "source": [
    "当提供一些训练数据时，我们未经训练的网络可能不会给出正确的答案。**损失函数**衡量的是得到的结果与目标值的不相似程度，它是我们在训练时想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。\r\n",
    "\r\n",
    "常见的损失函数包括用于回归任务的[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)（Mean Square Error 均方误差）和 用于分类的[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)（Negative Log Likelihood 负对数似然）。 [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)结合了`nn.LogSoftmax`和`nn.NLLLoss`。\r\n",
    "\r\n",
    "我们将模型的输出 logits 传递给`nn.CrossEntropyLoss`，这将标准化 logits 并计算预测误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e13731-5333-4c07-8bd9-a6847a6558f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb188e4-27cf-4aa3-a739-461bb0be5d27",
   "metadata": {},
   "source": [
    "##  Optimizer 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e02b75-5a2f-4d22-84ad-4f1ed43534c6",
   "metadata": {},
   "source": [
    "优化是调整模型参数以减少每个训练步骤中模型误差的过程。**Optimization algorithms**定义了如何执行此过程（在本例中我们使用随机梯度下降）。所有优化逻辑都封装在`optimizer`对象中。这里，我们使用SGD优化器；此外，PyTorch 中还有许多[不同的优化器](https://pytorch.org/docs/stable/optim.html) ，例如 ADAM 和 RMSProp，它们可以更好地处理不同类型的模型和数据。\r\n",
    "\r\n",
    "注册需要训练的模型参数，并传入学习率超参数。我们通过这种方式，来初始化优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb578000-eac6-437a-b63b-e5f43ccda4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9696bc-d18b-4db7-bb91-0c84e5e3d304",
   "metadata": {},
   "source": [
    "在训练循环中，优化分三个步骤进行：\r\n",
    "\r\n",
    "- 调用`optimizer.zero_grad()`重置模型参数的梯度。默认情况下渐变相加；为了防止重复计算，我们在每次迭代时明确地将它们归零。\r\n",
    "- 通过调用`loss.backward()`来反向传播预测损失。PyTorch 存储每个参数的损失梯度。\r\n",
    "- 一旦我们有了梯度，通过后向传递中收集的梯度，我们就可以调用`optimizer.step()`来调整参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c87f9-5b5d-47e7-b7fb-9893bbc7b1db",
   "metadata": {},
   "source": [
    "# Full Implementation 全面实施"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76d7f6-cb14-42b6-961b-92d608bf595a",
   "metadata": {},
   "source": [
    "我们定义了`train_loop`优化代码的循环，`test_loop`根据我们的测试数据评估模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e668f182-c043-46ca-8082-d66cc5e7ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} {current:>5d}/{size:>5d}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "# Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd07bf0-b066-49a7-93f5-b38131b3aa9a",
   "metadata": {},
   "source": [
    "我们初始化损失函数和优化器，并将其传递给`train_loop`和`test_loop`。请随意增加epoch数来跟踪模型改进的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569db738-a004-4f89-8122-0d6e14a884bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " ----------\n",
      "loss: 2.302923    64/60000\n",
      "loss: 2.294933  6464/60000\n",
      "loss: 2.282137 12864/60000\n",
      "loss: 2.277195 19264/60000\n",
      "loss: 2.263870 25664/60000\n",
      "loss: 2.244094 32064/60000\n",
      "loss: 2.249822 38464/60000\n",
      "loss: 2.220745 44864/60000\n",
      "loss: 2.217016 51264/60000\n",
      "loss: 2.199368 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.188615 \n",
      "\n",
      "Epoch 2 \n",
      " ----------\n",
      "loss: 2.191007    64/60000\n",
      "loss: 2.184882  6464/60000\n",
      "loss: 2.141239 12864/60000\n",
      "loss: 2.154306 19264/60000\n",
      "loss: 2.115879 25664/60000\n",
      "loss: 2.062999 32064/60000\n",
      "loss: 2.099697 38464/60000\n",
      "loss: 2.027198 44864/60000\n",
      "loss: 2.030211 51264/60000\n",
      "loss: 1.982426 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.966249 \n",
      "\n",
      "Epoch 3 \n",
      " ----------\n",
      "loss: 1.986138    64/60000\n",
      "loss: 1.963718  6464/60000\n",
      "loss: 1.865006 12864/60000\n",
      "loss: 1.902561 19264/60000\n",
      "loss: 1.801050 25664/60000\n",
      "loss: 1.743105 32064/60000\n",
      "loss: 1.786070 38464/60000\n",
      "loss: 1.676546 44864/60000\n",
      "loss: 1.699915 51264/60000\n",
      "loss: 1.610217 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.602007 \n",
      "\n",
      "Epoch 4 \n",
      " ----------\n",
      "loss: 1.659835    64/60000\n",
      "loss: 1.618469  6464/60000\n",
      "loss: 1.478276 12864/60000\n",
      "loss: 1.544722 19264/60000\n",
      "loss: 1.419785 25664/60000\n",
      "loss: 1.407439 32064/60000\n",
      "loss: 1.436413 38464/60000\n",
      "loss: 1.348989 44864/60000\n",
      "loss: 1.381547 51264/60000\n",
      "loss: 1.284938 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.293940 \n",
      "\n",
      "Epoch 5 \n",
      " ----------\n",
      "loss: 1.370868    64/60000\n",
      "loss: 1.344686  6464/60000\n",
      "loss: 1.191302 12864/60000\n",
      "loss: 1.286089 19264/60000\n",
      "loss: 1.159948 25664/60000\n",
      "loss: 1.180653 32064/60000\n",
      "loss: 1.207715 38464/60000\n",
      "loss: 1.139596 44864/60000\n",
      "loss: 1.176541 51264/60000\n",
      "loss: 1.094215 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 1.102279 \n",
      "\n",
      "Epoch 6 \n",
      " ----------\n",
      "loss: 1.175796    64/60000\n",
      "loss: 1.172489  6464/60000\n",
      "loss: 1.004980 12864/60000\n",
      "loss: 1.129330 19264/60000\n",
      "loss: 0.999406 25664/60000\n",
      "loss: 1.030680 32064/60000\n",
      "loss: 1.069462 38464/60000\n",
      "loss: 1.008194 44864/60000\n",
      "loss: 1.046673 51264/60000\n",
      "loss: 0.979699 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.981694 \n",
      "\n",
      "Epoch 7 \n",
      " ----------\n",
      "loss: 1.043684    64/60000\n",
      "loss: 1.063869  6464/60000\n",
      "loss: 0.879881 12864/60000\n",
      "loss: 1.028253 19264/60000\n",
      "loss: 0.899682 25664/60000\n",
      "loss: 0.927432 32064/60000\n",
      "loss: 0.980477 38464/60000\n",
      "loss: 0.923138 44864/60000\n",
      "loss: 0.958901 51264/60000\n",
      "loss: 0.905344 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.901349 \n",
      "\n",
      "Epoch 8 \n",
      " ----------\n",
      "loss: 0.948694    64/60000\n",
      "loss: 0.990619  6464/60000\n",
      "loss: 0.792132 12864/60000\n",
      "loss: 0.958839 19264/60000\n",
      "loss: 0.834637 25664/60000\n",
      "loss: 0.853436 32064/60000\n",
      "loss: 0.919122 38464/60000\n",
      "loss: 0.866312 44864/60000\n",
      "loss: 0.896762 51264/60000\n",
      "loss: 0.853390 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.844667 \n",
      "\n",
      "Epoch 9 \n",
      " ----------\n",
      "loss: 0.877188    64/60000\n",
      "loss: 0.936882  6464/60000\n",
      "loss: 0.727578 12864/60000\n",
      "loss: 0.908509 19264/60000\n",
      "loss: 0.789283 25664/60000\n",
      "loss: 0.798893 32064/60000\n",
      "loss: 0.873432 38464/60000\n",
      "loss: 0.826520 44864/60000\n",
      "loss: 0.850761 51264/60000\n",
      "loss: 0.814598 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.802346 \n",
      "\n",
      "Epoch 10 \n",
      " ----------\n",
      "loss: 0.820920    64/60000\n",
      "loss: 0.894438  6464/60000\n",
      "loss: 0.677681 12864/60000\n",
      "loss: 0.870346 19264/60000\n",
      "loss: 0.755491 25664/60000\n",
      "loss: 0.757496 32064/60000\n",
      "loss: 0.837215 38464/60000\n",
      "loss: 0.797053 44864/60000\n",
      "loss: 0.815341 51264/60000\n",
      "loss: 0.783963 57664/60000\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.769091 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} \\n ----------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad19cd6-e31b-42ff-b093-8df9cc7cfc21",
   "metadata": {},
   "source": [
    "# Further Reading 进一步阅读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41baf4-64fa-46ce-821e-3dd828662e12",
   "metadata": {},
   "source": [
    "- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\r\n",
    "- [torch.optim](https://pytorch.org/docs/stable/optim.html)\r\n",
    "- [Warmstart Training a Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a928b9e-7cab-472c-b9d4-0cad536b77f7",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1180e-be1e-40b3-8bfb-91bf1254433d",
   "metadata": {},
   "source": [
    "Optimizing Model Parameters — PyTorch Tutorials 2.2.0+cu121 documentation\n",
    "\n",
    "[Optimizing Model Parameters — PyTorch Tutorials 2.2.0+cu121 documentation](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#further-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1c4bd-84a9-4bbc-af43-50526bca642f",
   "metadata": {},
   "source": [
    "# Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0098687-4ba4-41bd-95f6-cf7157453ed5",
   "metadata": {},
   "source": [
    "storm-ice/Get_started_with_PyTorch\n",
    "\n",
    "[storm-ice/Get_started_with_PyTorch](https://github.com/storm-ice/Get_started_with_PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a92643-5aa3-465d-a754-48bac0f7b973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
